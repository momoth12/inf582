{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "In this session we will learn the following:\n",
    "- How to use NLTK to build n-gram Language Models\n",
    "- Build a Recurrent Neural Language Model\n",
    "- Apply Language Models to generation.\n",
    "\n",
    "## Preliminary Steps\n",
    "\n",
    "Make sure you have the latest NLTK version (3.4 or higher).\n",
    "You can install NLTK using pip: pip install NLTK\n",
    "\n",
    "The following examples will be using Python 3 syntax and conventions.\n",
    "Once you have installed NLTK, you need to download the required corpora. Launch the Python interpreter and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new window should open, showing the NLTK Downloader. Next, select all-corpora to download.\n",
    "\n",
    "NLTK provides many corpora covering different types of texts. We’ll work with the C-span corpus of state of the union and inaugural speeches by US presidents.\n",
    "\n",
    "To access the corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "\n",
    "\n",
    "#To list all documents in a corpus, you can use the fileids() method:\n",
    "inaugural.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#To access the content of a given document, you can use the raw(), words() and sents() methods as follows:\n",
    "inaugural.raw('2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inaugural.words('2009-Obama.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inaugural.sents('2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LM Module: Basics\n",
    "\n",
    "Since NLTK 3.4, the **lm** module allows to build language models.\n",
    "First thing we need is to count types (words, bigrams, trigrams, etc.): we can do this in different ways, using Counter from the collections package or, more properly, by using NgramCounter from the lm module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's count all unigrams (single words) in the State of the Union corpus (state_union):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text_unigrams = [ngrams(sent, 1) for sent in state_union.sents()]\n",
    "#for tu in text_unigrams:\n",
    "#    print(list(tu))\n",
    "ngram_counts=NgramCounter(text_unigrams)\n",
    "ngram_counts.N()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be careful since the ngrams function produces a generator: once the ngrams are used, they are 'lost'.\n",
    "\n",
    "You can look at the frequencies of a type (word) in a very simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ngram_counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ngram_counts.unigrams.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have matplotlib installed, it is possible to display a rank/frequency diagram by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ngram_counts.unigrams.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text_bigrams = [ngrams(sent, 2) for sent in state_union.sents()]\n",
    "ngram_counts=NgramCounter(text_bigrams)\n",
    "ngram_counts[['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ngram_counts[['the']]['people']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary objects allow to create a vocabulary from a set of types and a frequency threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "\n",
    "vocab = Vocabulary(state_union.words(), unk_cutoff=2)\n",
    "#The vocabulary will include all words that appear at least 2 times in the corpus.\n",
    "\n",
    "vocab[\"America\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Count the number of unigrams for each of the presidents in the inaugural dataset. Which one held the longest discourse? Which one the shortest one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "#Let's see all the presidential inaugural addresses that are available in the corpus:\n",
    "\n",
    "president_fileid= inaugural.fileids()\n",
    "unigram_counter={}\n",
    "for fileid in president_fileid:\n",
    "    current_president_date=fileid.split('.')[0]\n",
    "    text_unigrams = [ngrams(sent, 1) for sent in inaugural.sents(fileid)]\n",
    "    ngram_counts=NgramCounter(text_unigrams)\n",
    "    unigram_counter[current_president_date]=ngram_counts.N()\n",
    "    \n",
    "unigram_counter=sorted(unigram_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "a,b=unigram_counter[0][0].split('-')[0],unigram_counter[0][0].split('-')[1]\n",
    "c,d=unigram_counter[-1][0].split('-')[0],unigram_counter[-1][0].split('-')[1]\n",
    "print(f\"The president with the most words in his inaugural address is {b} in year {a} with {unigram_counter[0][1]} words\")\n",
    "print(f\"The president with the least words in his inaugural address is {d} in year {c} with {unigram_counter[-1][1]} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b)**: Count the number of different *types*. Which president used the \"richest vocabulary\" for his speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "type_counter={}\n",
    "for fileid in president_fileid:\n",
    "    current_president_date=fileid.split('.')[0]\n",
    "    text_unigrams = [ngrams(sent, 1) for sent in inaugural.sents(fileid)]\n",
    "    ngram_counts=NgramCounter(text_unigrams)\n",
    "    type_counter[current_president_date]=len(ngram_counts.unigrams)\n",
    "#normalized_type_counter={k: v/unigram_counter[k] for k, v in type_counter.items()}\n",
    "type_counter=sorted(type_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"The president with the most unique words in his inaugural address is {type_counter[0][0]} with {type_counter[0][1]} unique words\")\n",
    "print(f\"The president with the least unique words in his inaugural address is {type_counter[-1][0]} with {type_counter[-1][1]} unique words\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a=inaugural.raw('1793-Washington.txt').split()\n",
    "print(len(set(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a language model\n",
    "\n",
    "Once we have the data we need to introduce padding to tell the model what the boundaries of the sentence are, and to calculate probabilities for the starting and the end of a sentence. Luckily, NLTK has a function that allows us to do it easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import bigrams\n",
    "\n",
    "sentence=inaugural.sents('2009-Obama.txt')[1]\n",
    "\n",
    "list(pad_both_ends(sentence, n=2))\n",
    "list(bigrams(pad_both_ends(sentence, n=2))) #extract bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module lm will need also a flattened list of symbols to build the vocabulary. We can do it with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in inaugural.sents('2009-Obama.txt')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a convenience method called everygram_pipeline that produces the two at the same time – from the manual:\n",
    "    \n",
    "    padded_everygram_pipeline(order, text):\n",
    "    Default preprocessing for a sequence of sentences.\n",
    "\n",
    "    Creates two iterators:\n",
    "    - sentences padded and turned into sequences of `nltk.util.everygrams`\n",
    "    - sentences padded as above and chained together for a flat stream of words\n",
    "\n",
    "    :param order: Largest ngram length produced by `everygrams`.\n",
    "    :param text: Text to iterate over. Expected to be an iterable of sentences:\n",
    "    Iterable[Iterable[str]]\n",
    "    :return: iterator over text as ngrams, iterator over text as vocabulary data\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(3, inaugural.sents())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "lm = MLE(3) #the parameter is the highest n-gram order for our model. We consider up to trigrams in this example \n",
    "\n",
    "lm.fit(train, vocab) #fit with the data. May take a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify frequencies in the same way as we did with the NgramCounter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lm.counts['America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lm.counts[['bless']]['America']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function returns the probability of observing the given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#lm.score('America')\n",
    "#lm.score('America', ['bless']) #or the probability of observing a word given the previous word\n",
    "lm.score('America', ['God', 'bless'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these probabilities are not smoothed since we used a MLE model. For better results, models with smoothing are available:\n",
    "-\tnltk.lm.Lidstone (requires the gamma parameter to increase scores)\n",
    "-\tnltk.lm.Laplace (add 1)\n",
    "-\tnltk.lm.KneserNeyInterpolated\n",
    "\n",
    "**Exercise 2**: Build a language model from the **state_union** dataset. Verify the probabilities for the words \"America\", \"the\" and \"jobs\", first without smoothing and then using Laplace smoothing (warning: it may take a certain time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "#Let's generate some text using the trained model:\n",
    "from nltk.lm import Laplace\n",
    "\n",
    "\n",
    "state_union_sentences=state_union.sents()\n",
    "train, vocab = padded_everygram_pipeline(3, state_union_sentences)\n",
    "lm=MLE(3)\n",
    "lm_laplace=Laplace(3)\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "print(\"The probabilities without smoothing are:\")\n",
    "print(f\"America: {lm.score('America')}\")\n",
    "print(f\"the: {lm.score('the')}\")\n",
    "print(f\"jobs: {lm.score('jobs')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "state_union_sentences=state_union.sents()\n",
    "train, vocab = padded_everygram_pipeline(3, state_union_sentences)\n",
    "lm_laplace=Laplace(3)\n",
    "lm_laplace.fit(train, vocab)\n",
    "print(\"The probabilities with smoothing are:\")\n",
    "print(f\"America: {lm.score('America')}\")\n",
    "print(f\"the: {lm.score('the')}\")\n",
    "print(f\"jobs: {lm.score('jobs')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating language models: perplexity\n",
    "\n",
    "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = 2^{-\\frac{1}{N} \\left( \\sum_{t=1}^N \\log P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)},\n",
    "$$\n",
    "\n",
    "\n",
    "**Exercise 3**: We would like to create a function that calculates the perplexity on a given test set, made of multiple sentences, returning their average. Complete the following code to calculate the perplexity as defined above\n",
    "\n",
    "Hint: you can obtain the log-probabilities from the model with the function lm.logscore(...). To help, we include the conversion of the input sentences into sequences of n-grams, including the start and the end of the sentences (special symbols \\<s> and \\</s> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "def perplexity(lm, sents, n, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param sents: a list of sentences (each sentence a list of words)\n",
    "    :param n: the size of n-grams for which to compute the perplexity. This cannot exceed the size used for the construction of the LM\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprob, set it equal to min_logprob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \"\"\"\n",
    "    \n",
    "    test_data = [nltk.ngrams(t, n, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in sents]\n",
    "    prp=[]\n",
    "\n",
    "    for test in test_data:\n",
    "        logprob=0\n",
    "        for ngram in test:\n",
    "            context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "            logprob+=lm.score(word, context)\n",
    "        prp.append(logprob)\n",
    "        \n",
    "    return np.mean(prp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.a**: Evaluate your perplexity function on the *inaugural* dataset and test for $n \\in \\{1,2,3,4\\}$. What do you obtain? Can you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#example\n",
    "sents=inaugural.sents()\n",
    "\n",
    "perplexity(lm, sents, 3)\n",
    "#YOUR \n",
    "for n in range(1, 5):\n",
    "    print(f\"Perplexity for n={n}:{perplexity(lm, sents, n)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Finally, we are using the model to generate language. The key function is called **generate**, always in the lm interface\n",
    "\n",
    "    def generate(self, num_words=1, text_seed=None, random_seed=None):\n",
    "        Generate words from the model.\n",
    "\n",
    "        :param int num_words: How many words to generate. By default 1.\n",
    "        :param text_seed: Generation can be conditioned on preceding context.\n",
    "        :param random_seed: If provided, makes the random sampling part of\n",
    "        generation reproducible.\n",
    "        :return: One (str) word or a list of words generated from model.\n",
    " \n",
    "**Exercise 4**: Generate 10 sentences, each composed by 10 words, using the prompt \"I shall\", and calculate the average perplexity on the generated set. Compare this value to the value obtained on the *inaugural* dataset. Try for 2- and 3- grams.\n",
    "\n",
    "What can you conclude about the quality of the generated text (both on the basis of the values you obtained and your personal judgment)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lm.generate(10, [\"I\", \"shall\"])\n",
    "#YOUR CODE HERE\n",
    "def generate(self, num_words=1, text_seed=None,random_seed=None):\n",
    "    \n",
    "    if text_seed is None:\n",
    "        text_seed = [\"<s>\"] * (self.order - 1)\n",
    "    return self._generate(num_words, text_seed)\n",
    "\n",
    "\n",
    "prompt=[\"I shall\"]\n",
    "#Lets generate 10 sentences each with 10 words using the prompt \"I shall\"\n",
    "sentences=[]\n",
    "for i in range(10):\n",
    "    sentences.append(' '.join(lm.generate(10, prompt)))\n",
    "\n",
    "for s in sentences:\n",
    "    print(s)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Models\n",
    "\n",
    "The following script contains a demonstration of how to create a neural language model using Recurrent NN (in this case, LSTM) with Keras. Word vectors are one-hot representations. This script has an embedded training text, which is too short to produce reliable results (as you will probably notice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "    #the purpose of this function is to transform the text in a format that can be handled by the model\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in corpus: #process corpus one line at a time\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len, total_words\n",
    "\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len)) #Input Layer : Takes the sequence of words as input\n",
    "    model.add(LSTM(150)) #LSTM Layer : Computes the output using LSTM units.\n",
    "    model.add(Dropout(0.5)) #Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting.\n",
    "    model.add(Dense(total_words, activation='softmax')) #Output Layer : Computes the probability of the best possible next word as output\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.fit(predictors, label, epochs=500, verbose=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell has some text that is used to train the model; use the short text to take a look at how the testing works, and the 10 discourses (initially commented) from the inaugural dataset for the final experiments. If this takes too long, you can reduce the number of epochs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = \"\"\"The cat and her kittens\n",
    "They put on their mittens,\n",
    "To eat a Christmas pie.\n",
    "The poor little kittens\n",
    "They lost their mittens,\n",
    "And then they began to cry.\n",
    "O mother dear, we sadly fear\n",
    "We cannot go to-day,\n",
    "For we have lost our mittens.\n",
    "If it be so, ye shall not go,\n",
    "For ye are naughty kittens.\n",
    "The three little kittens, they found their mittens,\n",
    "And they began to cry,\n",
    "Oh, mother dear, see here, see here,\n",
    "For we have found our mittens.\n",
    "Put on your mittens, you silly kittens,\n",
    "And you shall have some pie.\n",
    "Purr, purr, purr,\n",
    "Oh, let us have some pie.\n",
    "The three little kittens,\n",
    "they washed their mittens,\n",
    "And hung them out to dry,\n",
    "Oh, mother dear, do you not hear,\n",
    "That we have washed our mittens?\n",
    "What, washed your mittens,\n",
    "then you're good kittens,\n",
    "But I smell a rat close by.\n",
    "Meow, meow, meow,\n",
    "We smell a rat close by.\"\"\"\n",
    "\n",
    "#data = '\\n'.join([' '.join(s) for s in inaugural.sents()[:10]])\n",
    "\n",
    "X, Y, msl, total_words = dataset_preparation(data)\n",
    "model = create_model(X, Y, msl, total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating with Temperature\n",
    "\n",
    "As seen in the course, temperature can be used to tune the creativity of the model.\n",
    "\n",
    "In the example below, we are not using temperature for sampling as the generate_text function always returns the most probable item.\n",
    "\n",
    "As the model output are probabilities and not the logits (the softmax has been already applied), we use a trick to calculate the temperature on the final result: we use log to reverse the softmax operation and get logit-like values:\n",
    "\n",
    "$$e^{(log(a)/T)} = a^{(1/T)}$$\n",
    "\n",
    "**Exercise 5**: Modify the generate_text function to use temperatures to sample and test the results with temperature temp=2 and temp 0.2. Hint: you can use the function np.random.choice(...) to sample, use as parameters the list of indices of words and as p the probabilities with temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len, model, temp=0):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=\n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        #here we obtain the index of the predicted word\n",
    "        #note that model.predict(...) returns the probabilities associated to the words\n",
    "        if temp==0: #if Temperature == 0, then return the most probable token\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        else:\n",
    "            probs=model.predict(token_list)\n",
    "            #YOUR CODE HERE\n",
    "            predicted = np.random.choice(range(len(probs[0])), p=probs[0])\n",
    "            \n",
    "            #Hint: predicted = np.random.choice(...)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            #we look for the index in the dictionary created by the tokenizer, then we get the word\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = generate_text(\"I stand here today humbled by the task before us\", 30, msl, model, 0.2)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative sampling strategies\n",
    "\n",
    "__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n",
    "\n",
    "Suppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize: \n",
    "$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n",
    "\n",
    "__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleus sampling, we sample from top-__N%__ fraction of the probability mass.\n",
    "\n",
    "Using the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleus N=0.9, the nucleus words consist of:\n",
    "1. most likely token $w_2$, because $p(w_2) < N$\n",
    "2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n",
    "3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n",
    "\n",
    "And thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n",
    "\n",
    "After you've selected the nucleous words, you need to re-normalize them as in top-k sampling and generate the next token.\n",
    "\n",
    "**Exercise 6**: Implement a generate_with_nucleus_sampling function to use the nucleus sampling strategy. Compare (qualitatively) the results obtained with nucleus=0.9 and nucleus=0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_with_nucleus_sampling(seed_text, next_words, max_sequence_len, model, nucleus=0):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=\n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        #here we obtain the index of the predicted word\n",
    "        #note that model.predict(...) returns the probabilities associated to the words\n",
    "        if nucleus==0: #if nucleus == 0, then return the most probable token\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        else:\n",
    "            probs=model.predict(token_list)\n",
    "            #YOUR CODE\n",
    "            #...\n",
    "            probs=probs[0]\n",
    "            sorted_indexes=np.argsort(probs)[::-1]\n",
    "            cumulative_probs=np.cumsum(probs[sorted_indexes])\n",
    "            nucleus_indexes=np.where(cumulative_probs>nucleus)[0]\n",
    "            if len(nucleus_indexes)==0:\n",
    "                taken_indexes=[sorted_indexes[0]]\n",
    "            else:\n",
    "                taken_indexes=sorted_indexes[:nucleus_indexes[-1] + 1]\n",
    "            selected_probabilities=probs[taken_indexes]/np.sum(probs[taken_indexes])  \n",
    "            predicted = np.random.choice(taken_indexes, p=selected_probabilities,size=1)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            #we look for the index in the dictionary created by the tokenizer, then we get the word\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = generate_with_nucleus_sampling(\"I stand here today humbled by the task before us\", 30, msl, model, 0.9)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation using GPT-2\n",
    "\n",
    "The following script uses a **pre-trained** GPT-2 model to generate texts. This model has been trained on a vast set of documents scraped from the web.\n",
    "\n",
    "You can use this script to produce a text based on an excerpt from a discourse in a database.\n",
    "\n",
    "**Exercise 7**: Run this script and observe the result. Does the result look like an US inaugural or state of the union address? How would you adapt the model to produce a realistic US inaugural or state of the union address?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mNameError: name 'inaugural' is not defined. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "text = \"I stand here today humbled by the task before us\"\n",
    "\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "encoded_prompt = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoded_prompt = encoded_prompt.to(torch.device(\"cpu\"))\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=200, #number of tokens that will be produced (includes seed)\n",
    "    temperature=0.9, #regulates \"creativity of the model\" - 1.0 default\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0, #default values\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Batch size == 1. to add more examples please use num_return_sequences > 1\n",
    "generated_sequence = output_sequences[0].tolist()\n",
    "text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
