{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIAW MOUHAMADOU LAMINE BARA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labelling\n",
    "\n",
    "In this session we will build an HMM model for PoS-tagging and then CRF and neural models for Named Entity Recognition.\n",
    "\n",
    "## Building a simple Hidden Markov Model\n",
    "\n",
    "In this first part of the lab we will build a very simple bigram HMM using probability estimates over the Brown corpus, which is Part-of-Speech tagged.\n",
    "\n",
    "Recall from course 6: probability estimates (with MLE estimation) can be calculated by dividing the number of occurrences of a bigram by the number of occurrences of the first word.\n",
    "\n",
    "First of all, we import the corpus where we will estimate the probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is in the form of sequences of sentences, where each sentence is made by a sequence of pairs (word, POS-tag), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall (from the course) that a Hidden Markov Model is composed by:\n",
    "\n",
    "- A set of $N$ states $Q = \\{q_1, q_2, \\ldots, q_N\\}$\n",
    "- A transition probability matrix $A=a_{11}\\ldots a_{ij} \\ldots a_{NN}$, where each $a_{ij}$ represents the probability of transitioning from state $q_i$ to $q_j$; note that $\\sum_{j=1}^N{a_{ij}} = 1 \\forall i$\n",
    "- A sequence of $T$ observations $O = o_1, o_2, \\ldots o_T$, each one drawn from a vocabulary of size $V=v_1, v_2, \\ldots, v_M$, of size $M$;\n",
    "- A sequence of *observation likelihoods* $B=b_i(o_t)$, also called **emission probabilities**, each expressing the probability of an observation $o_t$ being generated from a state $q_i$;\n",
    "- Finally, an initial probability distribution $\\Pi = \\pi_i, \\pi_2, \\ldots, \\pi_N$ where $\\pi_i$ indicates the probability that the Markov chain will start from state $q_i$. Some states $q_j$ may have $\\pi_j = 0$, meaning that they cannot be initial states. Also, $\\sum_{i=1}^N{\\pi_i}=1$.\n",
    "\n",
    "In our case, the set of states $Q$ is made by the vocabulary of labels (the POS-tags). The vocabulary $V$ corresponds to the word vocabulary (i.e. all the set of different words that appear in our corpus). The observations correspond to the sentences in the corpus.\n",
    "\n",
    "We will now split our corpus in a training and test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents()\n",
    "\n",
    "training = corpus[:-10]\n",
    "testing = corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Extract $Q$ and $V$ from the Brown corpus and determine their respective size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of Q in the corpus is 472\n",
      "The size of V in the corpus is 56057\n"
     ]
    }
   ],
   "source": [
    "#insert your solution here\n",
    "pos_tags= [tag for word, tag in brown.tagged_words()]\n",
    "Q=set(pos_tags)\n",
    "vocabulary= [word for word, tag in brown.tagged_words()]\n",
    "V=set(vocabulary)\n",
    "print(f\"The size of Q in the corpus is {len(Q)}\")\n",
    "print(f\"The size of V in the corpus is {len(V)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Create the matrices ($A$, $B$ and $\\Pi$) by using the probabilities estimated on the training set; since we are considering bigrams, the probabilities of the transition matrix will be calculated as $\\frac{count(t_{-1}, t)}{count(t_{-1})}$.\n",
    "\n",
    "*Important*: you will need to add smoothing (for instance Lidstone with $k=0.1$) on $B$ otherwise the output will be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your solution here\n",
    "\n",
    "#Expected output:\n",
    "#pi matrix such that pi[i] is the probability of starting in state q_i\n",
    "#A matrix (Q x Q sized) such that A[i][j] is the probability of moving from state q_i to state q_j\n",
    "#B matrix (Q x O sized) such that B[i][j] is the probability of state q_i to emit the word (observation) o_j\n",
    "Q=list(Q)\n",
    "V=list(V)\n",
    "import numpy as np\n",
    "n=len(Q)\n",
    "m=len(V)\n",
    "k=0.1\n",
    "\n",
    "A=np.zeros((n,n))\n",
    "B=0.1*np.ones((n,m))\n",
    "pi=np.zeros(n)\n",
    "index_Q=dict()\n",
    "index_V=dict()\n",
    "for i in range(n):\n",
    "    index_Q[Q[i]]=i\n",
    "for i in range(m):\n",
    "    index_V[V[i]]=i\n",
    "for sentence in training:\n",
    "    for i,sent in enumerate(sentence):\n",
    "        q,v=index_Q[sent[1]],index_V[sent[0]]\n",
    "        pi[q]+=1\n",
    "        B[q][v]+=1\n",
    "        if i<len(sentence)-1:\n",
    "            q2=Q.index(sentence[i+1][1])\n",
    "            A[q][q2]+=1\n",
    "\n",
    "pi=pi/np.sum(pi)\n",
    "A=A/np.sum(A,axis=1)[:,None]\n",
    "B=B/np.sum(B,axis=1)[:,None]            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a model and we can estimate its performance over the test set.\n",
    "\n",
    "To do this, we need the Viterbi algorithm for the decoding. To help you, an implementation of Viterbi is provided:\n",
    "(note: to use this version you need to assign a numeric id to each word and tag, if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params is a triple (pi, A, B) where\n",
    "pi = initial probability distribution over states\n",
    "A = transition probability matrix\n",
    "B = emission probability matrix\n",
    "\n",
    "observations is the sequence of observations (in our case, the observed words)\n",
    "\n",
    "the function returns the optimal sequence of states and its score\n",
    "\"\"\"\n",
    "def viterbi(params, observations):\n",
    "    pi, A, B = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf') #cases that have not been treated\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "\n",
    "    # base case\n",
    "    alpha[0, :] = pi * B[:,observations[0]]\n",
    "\n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "\n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
    "\n",
    "#Example:\n",
    "\n",
    "#original sentence: you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
    "#sentence as sequence of word indexes:\n",
    "#word_index=[42350, 44913, 3024, 50638, 15858, 16209, 36949, 31092, 28334, 45518, 22719, 26179, 32651, 52996, 25205, 16840, 36949, 1402, 46003, 10606, 19795, 3739]\n",
    "\n",
    "#predicted, score = viterbi((pi, A, B), word_index)\n",
    "\n",
    "#predicted will be a sequence of tag indexes:\n",
    "#[12, 55, 86, 39, 29, 4, 70, 7, 14, 7, 0, 6, 21, 28, 12, 55, 28, 27, 28, 0, 9, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      VB         VBD\n",
      "      IN         RP\n",
      "      IN         IN\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      VB         VB\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NP         .\n",
      "      .         .\n",
      "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         RB\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBG         VBG\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      FW-RB         VBN\n",
      "      FW-IN         TO\n",
      "      FW-NN         VB\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      NNS         NP\n",
      "      ,         ,\n",
      "      NP         PPSS\n",
      "      NN         VB\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NP         VBN-TL\n",
      "      NN         NNS-TL\n",
      "      ,         ,\n",
      "      NN         NN\n",
      "      NN         ''\n",
      "      ,         ,\n",
      "      NN         NP\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AP         AP\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         ``\n",
      "      PPSS+HV         PPSS+HV\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      VBN         VBN\n",
      "      RB         RB\n",
      "      .         .\n",
      "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      JJ         JJ\n",
      "      QLP         QLP\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEZ         BEZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBN         VBN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      VB         VB\n",
      "      PPO         PPO\n",
      "      RP         RP\n",
      "      --         --\n",
      "      PPS+BEZ         PPS+BEZ\n",
      "      PN         PN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  As you can count on me to do the same .\n",
      "   ##TRUE##    ##PRED##\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      MD         MD\n",
      "      VB         VB\n",
      "      IN         IN\n",
      "      PPO         PPO\n",
      "      TO         TO\n",
      "      DO         DO\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      .         .\n",
      "  The sentence is:  Compassionately yours ,\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         CS\n",
      "      PP$$         PP$$\n",
      "      ,         ,\n",
      "  The sentence is:  S. J. Perelman\n",
      "   ##TRUE##    ##PRED##\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "  The sentence is:  revulsion in the desert\n",
      "   ##TRUE##    ##PRED##\n",
      "      NN-HL         NN\n",
      "      IN-HL         IN\n",
      "      AT-HL         AT\n",
      "      NN-HL         NN\n",
      "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
      "   ##TRUE##    ##PRED##\n",
      "      AT         AT\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NP-TL         NN\n",
      "      NN         NN\n",
      "      VBD         VBD\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VBD         VBD\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      ,         ,\n",
      "      VBG         NP\n",
      "      ,         ,\n",
      "      VBD         VBD\n",
      "      RP         RP\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NNS\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      VBD         NN\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPS         PPS\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      VBG         VBG\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      --         --\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NNS         NNS\n",
      "      ,         ,\n",
      "      JJ         NP\n",
      "      ,         ,\n",
      "      WPS         WPS\n",
      "      VBD         VBD\n",
      "      AT         AT\n",
      "      NP         JJ\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      JJ-TL         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      WP$         WP$\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN$         NN$\n",
      "      VBG         NN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEDZ         BEDZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      JJR         JJR\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      QL         QL\n",
      "      JJ         JJ\n",
      "      .         .\n",
      "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
      "   ##TRUE##    ##PRED##\n",
      "      IN         IN\n",
      "      WDT         WDT\n",
      "      PPSS         PPSS\n",
      "      BEDZ         BEDZ\n",
      "      JJ         JJ\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         QL\n",
      "      NN         JJ\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      VBG         VBN\n",
      "      .         .\n"
     ]
    }
   ],
   "source": [
    "#Example of results calculation\n",
    "\n",
    "\n",
    "testing_formated = []\n",
    "for sentence in testing:\n",
    "    sent = \"\"\n",
    "    words_index = [] #vector of word indices to be passed to Viterbi\n",
    "    true_label= [] #vector of the true labels from labeled corpus\n",
    "    for word,tag in sentence:\n",
    "        words_index.append(index_V[word]) #word_to_index is a dictionary mapping a word to its index\n",
    "        true_label.append(tag)\n",
    "        sent=sent+\" \"+word\n",
    "    testing_formated.append((words_index,true_label,sent))\n",
    "\n",
    "\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    print(\"  The sentence is:\",sentence)\n",
    "    print(\"   ##TRUE##    ##PRED##\")\n",
    "    predicted, score = viterbi((pi, A, B), word_index) #call the viterbi decoder\n",
    "    for i,true_label in enumerate(labels):\n",
    "        predicted_label = Q[predicted[i]] #Q here is the vector of tags, so that at Q[i] we have the i_th tag in literal form\n",
    "        print(\"      \"+true_label+\"         \"+predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: calculate Precision, Recall and F-measure for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels=[]\n",
    "predicted_label = []\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    true_labels.append(labels)\n",
    "    predicted, score = viterbi((pi, A, B), word_index)\n",
    "    to_append=[]\n",
    "    for j,lab in enumerate(labels):\n",
    "        to_append.append(Q[predicted[j]])\n",
    "    predicted_label.append(to_append)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ''       0.00      0.00      0.00         0\n",
      "           ,       1.00      1.00      1.00        24\n",
      "          --       1.00      1.00      1.00         2\n",
      "           .       0.88      1.00      0.93         7\n",
      "          AP       1.00      1.00      1.00         4\n",
      "          AT       0.96      1.00      0.98        26\n",
      "       AT-HL       0.00      0.00      0.00         1\n",
      "          BE       1.00      1.00      1.00         2\n",
      "        BEDZ       1.00      1.00      1.00         5\n",
      "         BEZ       1.00      1.00      1.00         1\n",
      "          CC       1.00      1.00      1.00         7\n",
      "          CS       0.88      1.00      0.93         7\n",
      "          DO       1.00      1.00      1.00         1\n",
      "       FW-IN       0.00      0.00      0.00         1\n",
      "       FW-NN       0.00      0.00      0.00         1\n",
      "       FW-RB       0.00      0.00      0.00         1\n",
      "          IN       0.95      0.95      0.95        19\n",
      "       IN-HL       0.00      0.00      0.00         1\n",
      "          JJ       0.77      0.83      0.80        12\n",
      "       JJ-TL       0.00      0.00      0.00         1\n",
      "         JJR       1.00      1.00      1.00         1\n",
      "          MD       1.00      1.00      1.00         1\n",
      "         MD*       1.00      1.00      1.00         2\n",
      "          NN       0.84      0.79      0.82        34\n",
      "         NN$       1.00      1.00      1.00         1\n",
      "       NN-HL       0.00      0.00      0.00         2\n",
      "         NNS       0.80      0.80      0.80         5\n",
      "      NNS-TL       0.00      0.00      0.00         0\n",
      "          NP       0.43      0.43      0.43         7\n",
      "       NP-TL       0.00      0.00      0.00         1\n",
      "          PN       1.00      1.00      1.00         1\n",
      "         PP$       1.00      1.00      1.00         4\n",
      "        PP$$       1.00      1.00      1.00         1\n",
      "         PPO       1.00      1.00      1.00         2\n",
      "         PPS       1.00      1.00      1.00         1\n",
      "     PPS+BEZ       1.00      1.00      1.00         1\n",
      "        PPSS       0.88      1.00      0.93         7\n",
      "    PPSS+BER       1.00      1.00      1.00         2\n",
      "     PPSS+HV       1.00      1.00      1.00         1\n",
      "          QL       0.75      1.00      0.86         3\n",
      "         QLP       1.00      1.00      1.00         1\n",
      "          RB       1.00      0.80      0.89         5\n",
      "          RP       0.67      1.00      0.80         2\n",
      "          TO       0.83      1.00      0.91         5\n",
      "          VB       0.80      0.89      0.84         9\n",
      "         VBD       0.80      0.80      0.80         5\n",
      "         VBG       1.00      0.40      0.57         5\n",
      "         VBN       0.67      1.00      0.80         4\n",
      "      VBN-TL       0.00      0.00      0.00         0\n",
      "         WDT       1.00      1.00      1.00         1\n",
      "         WP$       1.00      1.00      1.00         1\n",
      "         WPS       1.00      1.00      1.00         1\n",
      "          ``       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       239\n",
      "   macro avg       0.71      0.73      0.72       239\n",
      "weighted avg       0.86      0.87      0.86       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concatenate the list of labels to pass to the classification report  \n",
    "print(classification_report(np.concatenate(true_labels),np.concatenate(predicted_label),zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: modify your HMM to use trigrams instead of bigrams, and re-evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouha\\AppData\\Local\\Temp\\ipykernel_21360\\4177155655.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  A=A/np.sum(A,axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPSS         PPSS\n",
      "      MD*         PPSS\n",
      "      QL         PPSS\n",
      "      RB         PPSS\n",
      "      VB         PPSS\n",
      "      IN         PPSS\n",
      "      IN         PPSS\n",
      "      NNS         PPSS\n",
      "      IN         PPSS\n",
      "      AT         PPSS\n",
      "      NN         PPSS\n",
      "      CC         PPSS\n",
      "      VB         PPSS\n",
      "      CS         PPSS\n",
      "      PPSS         PPSS\n",
      "      VB         PPSS\n",
      "      TO         PPSS\n",
      "      VB         PPSS\n",
      "      AT         PPSS\n",
      "      JJ         PPSS\n",
      "      NP         PPSS\n",
      "      .         AP\n",
      "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         AT\n",
      "      ,         NN\n",
      "      CS         NN\n",
      "      PPSS+BER         NN\n",
      "      VBG         NN\n",
      "      TO         NN\n",
      "      BE         NN\n",
      "      FW-RB         NN\n",
      "      FW-IN         NN\n",
      "      FW-NN         NN\n",
      "      QL         NN\n",
      "      RB         NN\n",
      "      IN         NN\n",
      "      NN         NN\n",
      "      ,         NN\n",
      "      NNS         NN\n",
      "      ,         NN\n",
      "      NP         NN\n",
      "      NN         NN\n",
      "      ,         NN\n",
      "      AT         NN\n",
      "      NP         NN\n",
      "      NN         NN\n",
      "      ,         NN\n",
      "      NN         NN\n",
      "      NN         NN\n",
      "      ,         NN\n",
      "      NN         NN\n",
      "      ,         NN\n",
      "      CC         NN\n",
      "      AT         NN\n",
      "      NN         NN\n",
      "      IN         NN\n",
      "      AP         NN\n",
      "      NNS         NN\n",
      "      IN         NN\n",
      "      AT         NN\n",
      "      NN         NN\n",
      "      PPSS+HV         NN\n",
      "      VBN         NN\n",
      "      ,         NN\n",
      "      PPSS         NN\n",
      "      MD*         NN\n",
      "      VB         NN\n",
      "      TO         NN\n",
      "      BE         NN\n",
      "      VBN         NN\n",
      "      RB         NN\n",
      "      .         AP\n",
      "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PP$         NP\n",
      "      NN         NP\n",
      "      ,         NP\n",
      "      CS         NP\n",
      "      PPSS         NP\n",
      "      VB         NP\n",
      "      JJ         NP\n",
      "      QLP         NP\n",
      "      TO         NP\n",
      "      VB         NP\n",
      "      PP$         NP\n",
      "      NN         NP\n",
      "      ,         NP\n",
      "      BEZ         NP\n",
      "      CS         NP\n",
      "      AT         NP\n",
      "      AP         NP\n",
      "      NN         NP\n",
      "      PPSS+BER         NP\n",
      "      VBN         NP\n",
      "      IN         NP\n",
      "      AT         NP\n",
      "      JJ         NP\n",
      "      ,         NP\n",
      "      VB         NP\n",
      "      PPO         NP\n",
      "      RP         NP\n",
      "      --         NP\n",
      "      PPS+BEZ         NP\n",
      "      PN         NP\n",
      "      CC         NP\n",
      "      AT         NP\n",
      "      NN         NP\n",
      "      .         .\n",
      "  The sentence is:  As you can count on me to do the same .\n",
      "   ##TRUE##    ##PRED##\n",
      "      CS         CS\n",
      "      PPSS         CS\n",
      "      MD         CS\n",
      "      VB         CS\n",
      "      IN         CS\n",
      "      PPO         CS\n",
      "      TO         CS\n",
      "      DO         CS\n",
      "      AT         CS\n",
      "      AP         CS\n",
      "      .         AP\n",
      "  The sentence is:  Compassionately yours ,\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         AT\n",
      "      PP$$         NN\n",
      "      ,         ,\n",
      "  The sentence is:  S. J. Perelman\n",
      "   ##TRUE##    ##PRED##\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "      NP         ,\n",
      "  The sentence is:  revulsion in the desert\n",
      "   ##TRUE##    ##PRED##\n",
      "      NN-HL         NN\n",
      "      IN-HL         NN\n",
      "      AT-HL         NN\n",
      "      NN-HL         AP\n",
      "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
      "   ##TRUE##    ##PRED##\n",
      "      AT         AT\n",
      "      NNS         AT\n",
      "      IN         AT\n",
      "      AT         AT\n",
      "      NP-TL         AT\n",
      "      NN         AT\n",
      "      VBD         AT\n",
      "      VBN         AT\n",
      "      ,         AT\n",
      "      CC         AT\n",
      "      CS         AT\n",
      "      PPSS         AT\n",
      "      VBD         AT\n",
      "      IN         AT\n",
      "      AT         AT\n",
      "      NN         AT\n",
      "      CC         AT\n",
      "      ,         AT\n",
      "      VBG         AT\n",
      "      ,         AT\n",
      "      VBD         AT\n",
      "      RP         AT\n",
      "      IN         AT\n",
      "      AT         AT\n",
      "      NN         AT\n",
      "      ,         AT\n",
      "      AT         AT\n",
      "      JJ         AT\n",
      "      NN         AT\n",
      "      IN         AT\n",
      "      PP$         AT\n",
      "      NN         AT\n",
      "      VBD         AT\n",
      "      IN         AT\n",
      "      NN         AT\n",
      "      .         AP\n",
      "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPS         PPS\n",
      "      BEDZ         PPS\n",
      "      AT         PPS\n",
      "      VBG         PPS\n",
      "      NN         PPS\n",
      "      CC         PPS\n",
      "      AT         PPS\n",
      "      NN         PPS\n",
      "      --         PPS\n",
      "      AT         PPS\n",
      "      JJ         PPS\n",
      "      NN         PPS\n",
      "      ,         PPS\n",
      "      AT         PPS\n",
      "      JJ         PPS\n",
      "      NNS         PPS\n",
      "      ,         PPS\n",
      "      JJ         PPS\n",
      "      ,         PPS\n",
      "      WPS         PPS\n",
      "      VBD         PPS\n",
      "      AT         PPS\n",
      "      NP         PPS\n",
      "      NN         PPS\n",
      "      IN         PPS\n",
      "      PP$         PPS\n",
      "      JJ-TL         PPS\n",
      "      NN         PPS\n",
      "      ,         PPS\n",
      "      CC         PPS\n",
      "      AT         PPS\n",
      "      NN         PPS\n",
      "      WP$         PPS\n",
      "      AP         PPS\n",
      "      NN         PPS\n",
      "      ,         PPS\n",
      "      IN         PPS\n",
      "      AT         PPS\n",
      "      NN$         PPS\n",
      "      VBG         PPS\n",
      "      NN         PPS\n",
      "      ,         PPS\n",
      "      BEDZ         PPS\n",
      "      CS         PPS\n",
      "      AT         PPS\n",
      "      JJR         PPS\n",
      "      NN         PPS\n",
      "      BEDZ         PPS\n",
      "      AT         PPS\n",
      "      NN         PPS\n",
      "      QL         PPS\n",
      "      JJ         PPS\n",
      "      .         AP\n",
      "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
      "   ##TRUE##    ##PRED##\n",
      "      IN         IN\n",
      "      WDT         IN\n",
      "      PPSS         IN\n",
      "      BEDZ         IN\n",
      "      JJ         IN\n",
      "      IN         IN\n",
      "      NN         IN\n",
      "      IN         IN\n",
      "      AT         IN\n",
      "      JJ         IN\n",
      "      ,         IN\n",
      "      JJ         IN\n",
      "      NN         IN\n",
      "      ,         IN\n",
      "      AT         IN\n",
      "      NN         IN\n",
      "      IN         IN\n",
      "      AT         IN\n",
      "      JJ         IN\n",
      "      NN         IN\n",
      "      NN         IN\n",
      "      BEDZ         IN\n",
      "      VBG         IN\n",
      "      .         AP\n"
     ]
    }
   ],
   "source": [
    "#Let me modify the hmm to use trigrams\n",
    "#i will first recode the viterbi function to accept a trigram model\n",
    "\n",
    "def concatenation_string(a, b):\n",
    "    return a + ' ' + b\n",
    "new_corpus=np.concatenate(corpus)\n",
    "Q=list(set([concatenation_string(new_corpus[ct][1], new_corpus[ct+1][1]) for ct in range(len(new_corpus)-1)]))\n",
    "\n",
    "\n",
    "\n",
    "n=len(Q)\n",
    "m=len(V)\n",
    "k=0.1\n",
    "\n",
    "A=np.zeros((n,n))\n",
    "B=0.1*np.ones((n,m))\n",
    "pi=np.zeros(n)\n",
    "index_Q=dict()\n",
    "index_V=dict()\n",
    "for i in range(n):\n",
    "    index_Q[Q[i]]=i\n",
    "for i in range(m):\n",
    "    index_V[V[i]]=i\n",
    "for sentence in training:\n",
    "    for i, s in enumerate(sentence[1:],1):\n",
    "        previous_st = sentence[i-1]\n",
    "        q = index_Q[concatenation_string(previous_st[1], s[1])]\n",
    "        v = index_V[s[0]]    \n",
    "        pi[q] += 1\n",
    "        B[q][v] += 1\n",
    "        if i < len(sentence)-1:\n",
    "            s_next = sentence[i+1]\n",
    "            q_next = Q.index(concatenation_string(s[1], s_next[1]))\n",
    "            A[q][q_next] += 1\n",
    "\n",
    "pi=pi/np.sum(pi)\n",
    "A=A/np.sum(A,axis=1)[:, np.newaxis]\n",
    "B=B/np.sum(B,axis=1)[:, np.newaxis]            \n",
    "\n",
    "\n",
    "\n",
    "p=2\n",
    "def viterbi_trigram(params, observations):\n",
    "    pi, A, B = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf') #cases that have not been treated\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "\n",
    "    # base case\n",
    "    alpha[0, :] = pi * B[:,observations[0]]\n",
    "    p_likely_states = np.argsort(-alpha[0, :])[:p]\n",
    "\n",
    "\n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in p_likely_states:\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "\n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
    "\n",
    "testing_formated = []\n",
    "for sentence in testing:\n",
    "    sent = \"\"\n",
    "    words_index = [] #vector of word indices to be passed to Viterbi\n",
    "    true_label= [] #vector of the true labels from labeled corpus\n",
    "    for word,tag in sentence:\n",
    "        words_index.append(index_V[word]) #word_to_index is a dictionary mapping a word to its index\n",
    "        true_label.append(tag)\n",
    "        sent=sent+\" \"+word\n",
    "    testing_formated.append((words_index,true_label,sent))\n",
    "\n",
    "\n",
    "\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    print(\"  The sentence is:\",sentence)\n",
    "    print(\"   ##TRUE##    ##PRED##\")\n",
    "    predicted, score = viterbi_trigram((pi, A, B), word_index) \n",
    "    for i,true_label in enumerate(labels):\n",
    "        predicted_label = Q[predicted[i]].split(\" \")[1] \n",
    "        print(\"      \"+true_label+\"         \"+predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ,       0.50      0.04      0.08        24\n",
      "          --       0.00      0.00      0.00         2\n",
      "           .       1.00      0.14      0.25         7\n",
      "          AP       0.00      0.00      0.00         4\n",
      "          AT       0.14      0.19      0.16        26\n",
      "       AT-HL       0.00      0.00      0.00         1\n",
      "          BE       0.00      0.00      0.00         2\n",
      "        BEDZ       0.00      0.00      0.00         5\n",
      "         BEZ       0.00      0.00      0.00         1\n",
      "          CC       0.00      0.00      0.00         7\n",
      "          CS       0.10      0.14      0.12         7\n",
      "          DO       0.00      0.00      0.00         1\n",
      "       FW-IN       0.00      0.00      0.00         1\n",
      "       FW-NN       0.00      0.00      0.00         1\n",
      "       FW-RB       0.00      0.00      0.00         1\n",
      "          IN       0.17      0.21      0.19        19\n",
      "       IN-HL       0.00      0.00      0.00         1\n",
      "          JJ       0.00      0.00      0.00        12\n",
      "       JJ-TL       0.00      0.00      0.00         1\n",
      "         JJR       0.00      0.00      0.00         1\n",
      "          MD       0.00      0.00      0.00         1\n",
      "         MD*       0.00      0.00      0.00         2\n",
      "          NN       0.16      0.24      0.19        34\n",
      "         NN$       0.00      0.00      0.00         1\n",
      "       NN-HL       0.00      0.00      0.00         2\n",
      "         NNS       0.00      0.00      0.00         5\n",
      "          NP       0.06      0.29      0.10         7\n",
      "       NP-TL       0.00      0.00      0.00         1\n",
      "          PN       0.00      0.00      0.00         1\n",
      "         PP$       0.00      0.00      0.00         4\n",
      "        PP$$       0.00      0.00      0.00         1\n",
      "         PPO       0.00      0.00      0.00         2\n",
      "         PPS       0.02      1.00      0.04         1\n",
      "     PPS+BEZ       0.00      0.00      0.00         1\n",
      "        PPSS       0.10      0.29      0.14         7\n",
      "    PPSS+BER       0.00      0.00      0.00         2\n",
      "     PPSS+HV       0.00      0.00      0.00         1\n",
      "          QL       0.00      0.00      0.00         3\n",
      "         QLP       0.00      0.00      0.00         1\n",
      "          RB       0.00      0.00      0.00         5\n",
      "          RP       0.00      0.00      0.00         2\n",
      "          TO       0.00      0.00      0.00         5\n",
      "          VB       0.00      0.00      0.00         9\n",
      "         VBD       0.00      0.00      0.00         5\n",
      "         VBG       0.00      0.00      0.00         5\n",
      "         VBN       0.00      0.00      0.00         4\n",
      "         WDT       0.00      0.00      0.00         1\n",
      "         WP$       0.00      0.00      0.00         1\n",
      "         WPS       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.10       239\n",
      "   macro avg       0.05      0.05      0.03       239\n",
      "weighted avg       0.14      0.10      0.08       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels=[]\n",
    "predicted_label = []\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    true_labels.append(labels)\n",
    "    predicted, score = viterbi_trigram((pi, A, B), word_index)\n",
    "    to_append=[]\n",
    "    for j,lab in enumerate(labels):\n",
    "        to_append.append(Q[predicted[j]].split(\" \")[1])\n",
    "    predicted_label.append(to_append)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.concatenate(true_labels),np.concatenate(predicted_label),zero_division=0))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK's HMM implementation\n",
    "\n",
    "We will compare now our model built from scratch to the implementation provided by NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VB'), ('up', 'IN'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', 'NP'), ('.', '.')]\n",
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VBD'), ('up', 'RP'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', '.'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'FW-RB'), ('de', 'FW-IN'), ('combat', 'FW-NN'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NNS'), (',', ','), ('Delhi', 'NP'), ('boil', 'NN'), (',', ','), ('the', 'AT'), ('Granville', 'NP'), ('wilt', 'NN'), (',', ','), ('liver', 'NN'), ('fluke', 'NN'), (',', ','), ('bilharziasis', 'NN'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', 'NN'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'VBN'), ('de', 'TO'), ('combat', 'VB'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NP'), (',', ','), ('Delhi', 'PPSS'), ('boil', 'VB'), (',', ','), ('the', 'AT'), ('Granville', 'VBN-TL'), ('wilt', 'NNS-TL'), (',', ','), ('liver', 'NN'), ('fluke', \"''\"), (',', ','), ('bilharziasis', 'NP'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', '``'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('Compassionately', 'RB'), ('yours', 'PP$$'), (',', ',')]\n",
      "[('Compassionately', '``'), ('yours', 'UH'), (',', ',')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('revulsion', 'NN-HL'), ('in', 'IN-HL'), ('the', 'AT-HL'), ('desert', 'NN-HL')]\n",
      "[('revulsion', 'NN'), ('in', 'IN'), ('the', 'AT'), ('desert', 'NN')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NP-TL'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'VBG'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NN'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'VBD'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NN'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'NP'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NNS'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'NN'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'JJ'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'NP'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ-TL'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'VBG'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'NP'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'JJ'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'NN'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'JJ'), ('boucle', 'NN'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBG'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'QL'), ('boucle', 'JJ'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states = Q, symbols = V)\n",
    "\n",
    "model = trainer.train_supervised(training, estimator=lambda fd, bins: hmm.LidstoneProbDist(fd, 0.1, bins))\n",
    "true_labels=[]\n",
    "preditions=[]\n",
    "for sent in testing:\n",
    "    u_sent=[]\n",
    "    tags=[]\n",
    "    for word, tag in sent:\n",
    "        u_sent.append(word)\n",
    "        tags.append(tag)\n",
    "    tagged=model.tag(u_sent)\n",
    "    print(sent)\n",
    "    print(tagged)\n",
    "    true_labels.append(tags)\n",
    "    prd=[t[1] for t in tagged]\n",
    "    preditions.append(prd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Calculate precision, recall and F-measure and compare them to the results that you obtained with the two models (bigram and trigram) that you implemented before. Can you deduce whether the NLTK model is using bigrams or trigrams? (It is not stated in the manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ''       0.00      0.00      0.00         0\n",
      "           ,       1.00      1.00      1.00        24\n",
      "          --       1.00      1.00      1.00         2\n",
      "           .       0.88      1.00      0.93         7\n",
      "          AP       1.00      1.00      1.00         4\n",
      "          AT       0.96      1.00      0.98        26\n",
      "       AT-HL       0.00      0.00      0.00         1\n",
      "          BE       1.00      1.00      1.00         2\n",
      "        BEDZ       1.00      1.00      1.00         5\n",
      "         BEZ       1.00      1.00      1.00         1\n",
      "          CC       1.00      1.00      1.00         7\n",
      "          CS       1.00      1.00      1.00         7\n",
      "          DO       1.00      1.00      1.00         1\n",
      "       FW-IN       0.00      0.00      0.00         1\n",
      "       FW-NN       0.00      0.00      0.00         1\n",
      "       FW-RB       0.00      0.00      0.00         1\n",
      "          IN       0.95      0.95      0.95        19\n",
      "       IN-HL       0.00      0.00      0.00         1\n",
      "          JJ       0.77      0.83      0.80        12\n",
      "       JJ-TL       0.00      0.00      0.00         1\n",
      "         JJR       1.00      1.00      1.00         1\n",
      "          MD       1.00      1.00      1.00         1\n",
      "         MD*       1.00      1.00      1.00         2\n",
      "          NN       0.84      0.79      0.82        34\n",
      "         NN$       1.00      1.00      1.00         1\n",
      "       NN-HL       0.00      0.00      0.00         2\n",
      "         NNS       0.80      0.80      0.80         5\n",
      "      NNS-TL       0.00      0.00      0.00         0\n",
      "          NP       0.43      0.43      0.43         7\n",
      "       NP-TL       0.00      0.00      0.00         1\n",
      "          PN       1.00      1.00      1.00         1\n",
      "         PP$       1.00      1.00      1.00         4\n",
      "        PP$$       0.00      0.00      0.00         1\n",
      "         PPO       1.00      1.00      1.00         2\n",
      "         PPS       1.00      1.00      1.00         1\n",
      "     PPS+BEZ       1.00      1.00      1.00         1\n",
      "        PPSS       0.88      1.00      0.93         7\n",
      "    PPSS+BER       1.00      1.00      1.00         2\n",
      "     PPSS+HV       1.00      1.00      1.00         1\n",
      "          QL       0.75      1.00      0.86         3\n",
      "         QLP       1.00      1.00      1.00         1\n",
      "          RB       1.00      0.80      0.89         5\n",
      "          RP       0.67      1.00      0.80         2\n",
      "          TO       0.83      1.00      0.91         5\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.80      0.89      0.84         9\n",
      "         VBD       0.80      0.80      0.80         5\n",
      "         VBG       1.00      0.40      0.57         5\n",
      "         VBN       0.67      1.00      0.80         4\n",
      "      VBN-TL       0.00      0.00      0.00         0\n",
      "         WDT       1.00      1.00      1.00         1\n",
      "         WP$       1.00      1.00      1.00         1\n",
      "         WPS       1.00      1.00      1.00         1\n",
      "          ``       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       239\n",
      "   macro avg       0.69      0.70      0.69       239\n",
      "weighted avg       0.86      0.87      0.86       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.concatenate(true_labels), np.concatenate(preditions), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Conditional Random Fields\n",
    "\n",
    "For this exercise we will need to use the sklearn_crfsuite package. If it is not installed, it can be installed using pip with ```pip install sklearn-crfsuite```.\n",
    "\n",
    "We will work on a Kaggle dataset named ```ner_dataset.csv``` (it should be in the same directory as the notebook).\n",
    "\n",
    "Pandas can be used to read the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouha\\AppData\\Local\\Temp\\ipykernel_32012\\3186707956.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emerged', 'hardcourt', 'Rafah', ',', 'Athanase', 'Wasserhoevel', 'one-point-three', 'caps', 'Shon', 'Phyu']\n",
      "35177\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n",
    "\n",
    "words = list(set(data[\"Word\"].values)) #vocabulary V\n",
    "n_words = len(words)\n",
    "\n",
    "print(words[:10])\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with some code that can read the sentences and produce the features in the format required by crf_suite. The ```SentenceGetter``` class transforms sentences into sequences of ```(word, POS, tag)``` triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouha\\AppData\\Local\\Temp\\ipykernel_32012\\985691969.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "#load data\n",
    "getter = SentenceGetter(data) #transform sentences into sequences of (Word, POS, Tag)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function allows us to define features that are used in the CRF. The features are stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    input:\n",
    "       sent: sentence in the format of sequence of (Word, POS, Tag) triples\n",
    "       i: position in the sentence\n",
    "    output:\n",
    "       features: a dictionary mapping the feature name into a value\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = { #features related to the current position\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0: #features related to preceding word/tag\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True #feature for Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1: #features related to the following word/tag\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True #feature for end of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    #transforms the sentence in a sequence of features\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    #transforms the sentence in a sequence of labels\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    #transforms the sentence in a sequence of tokens (removes POS tags and labels)\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the features and label vectors, and create a CRF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\mouha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\mouha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn_crfsuite) (0.9.10)\n",
      "Requirement already satisfied: six in c:\\users\\mouha\\appdata\\roaming\\python\\python310\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\mouha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\mouha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn_crfsuite) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mouha\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm='lbfgs',  max_iterations=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a model with gradient descent algorithm (\"lbfgs\") and a limit of $100$ iterations.\n",
    "\n",
    "Now we build the model and evaluate it on a 66/33 split between training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.40      0.01      0.03       137\n",
      "       B-eve       0.69      0.31      0.42       111\n",
      "       B-geo       0.83      0.91      0.87     12357\n",
      "       B-gpe       0.98      0.82      0.89      5226\n",
      "       B-nat       0.60      0.09      0.15        69\n",
      "       B-org       0.78      0.67      0.72      6762\n",
      "       B-per       0.83      0.79      0.81      5649\n",
      "       B-tim       0.93      0.84      0.88      6650\n",
      "       I-art       0.38      0.02      0.05       124\n",
      "       I-eve       0.56      0.21      0.31        89\n",
      "       I-geo       0.80      0.79      0.80      2433\n",
      "       I-gpe       0.95      0.33      0.49        55\n",
      "       I-nat       0.33      0.05      0.08        21\n",
      "       I-org       0.76      0.78      0.77      5545\n",
      "       I-per       0.85      0.87      0.86      5730\n",
      "       I-tim       0.81      0.74      0.78      2110\n",
      "           O       0.99      0.99      0.99    292571\n",
      "\n",
      "    accuracy                           0.97    345639\n",
      "   macro avg       0.73      0.54      0.58    345639\n",
      "weighted avg       0.97      0.97      0.96    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "crf.fit(X_train, y_train)\n",
    "pred=crf.predict(X_test)\n",
    "\n",
    "n_pred = [item for sublist in pred for item in sublist]\n",
    "n_test = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report shows accuracy stats for all classes, but we are not interested in the **O** class. We can see that the scores for people names, place names and organizations are relatively low.\n",
    "\n",
    "**Exercise 6**: Can you think of some new features for the CRF model to improve the results, especially on **B-org** ? Modify the *word2features* function to include the additional features and compare with the above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve results on B-org we can check if the word start with capital letter or is fully written with like in names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    input:\n",
    "       sent: sentence in the format of sequence of (Word, POS, Tag) triples\n",
    "       i: position in the sentence\n",
    "    output:\n",
    "       features: a dictionary mapping the feature name into a value\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = { #features related to the current position\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'postag': postag,\n",
    "        'start_capital': word[0].isupper(),\n",
    "        'all_caps': word.isupper(),\n",
    "    }\n",
    "    if i > 0: #features related to preceding word/tag\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True #feature for Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1: #features related to the following word/tag\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True #feature for end of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    #transforms the sentence in a sequence of features\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    #transforms the sentence in a sequence of labels\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    #transforms the sentence in a sequence of tokens (removes POS tags and labels)\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.24      0.03      0.05       137\n",
      "       B-eve       0.65      0.32      0.42       111\n",
      "       B-geo       0.83      0.91      0.87     12357\n",
      "       B-gpe       0.95      0.90      0.93      5226\n",
      "       B-nat       0.67      0.03      0.06        69\n",
      "       B-org       0.79      0.69      0.74      6762\n",
      "       B-per       0.83      0.79      0.81      5649\n",
      "       B-tim       0.93      0.84      0.88      6650\n",
      "       I-art       0.31      0.04      0.07       124\n",
      "       I-eve       0.50      0.21      0.30        89\n",
      "       I-geo       0.81      0.78      0.79      2433\n",
      "       I-gpe       0.96      0.45      0.62        55\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.77      0.80      0.78      5545\n",
      "       I-per       0.83      0.91      0.87      5730\n",
      "       I-tim       0.81      0.74      0.77      2110\n",
      "           O       0.99      0.99      0.99    292571\n",
      "\n",
      "    accuracy                           0.97    345639\n",
      "   macro avg       0.70      0.55      0.59    345639\n",
      "weighted avg       0.97      0.97      0.97    345639\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm='lbfgs',  max_iterations=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "crf.fit(X_train, y_train)\n",
    "pred=crf.predict(X_test)\n",
    "\n",
    "n_pred = [item for sublist in pred for item in sublist]\n",
    "n_test = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER using a LSTM model\n",
    "\n",
    "In this final section we will see an example of a neural network model written in PyTorch that uses a LSTM-based architecture for Named Entity Recognition.\n",
    "\n",
    "First of all, we will prepare the data to have all information coded numerically (words and tags) and the sentences padded to a max length, in order to have all sentences of the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of all we need to retrieve the number of different tags (a.k.a categories or classes of the words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags=len(tags)\n",
    "\n",
    "vocab = {w: i + 1 for i, w in enumerate(words)} #map words into a number\n",
    "tag_map = {t: i for i, t in enumerate(tags)} #map tags into a number\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len=75\n",
    "\n",
    "X = [[vocab[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words) #pad with special token PAD, with ID=n_words\n",
    "y = [[tag_map[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=-1) # -1 is associated to the PAD token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data and split them into training and test. We set batch size at 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NERDataset import NERDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "ner_train = NERDataset(X_train, y_train)\n",
    "ner_test = NERDataset(X_test, y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(ner_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model is defined here. We have an embedding that maps each word in a vector (embedding) of size 100, which is learnt from the dataset. The embedded sentence is fed to a LSTM layer of size 50. The output is transferred to a fully connected layer with *n_tags* output, one for each of the possible labels. The loss is a cross-entropy loss over all tokens (excluding the \"pad\" tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_size=n_words+1\n",
    "embedding_dim=100\n",
    "lstm_hidden_dim=50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, n_tags)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "        \n",
    "        #run the LSTM along the sentences of length batch_max_len. We discard the cell state as output\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "        \n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "        \n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "        \n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*batch_max_len x num_tags\n",
    "    \n",
    "def loss_fn(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "    \n",
    "    #discard 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "    \n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "    \n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block we carry out the training over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 1.678\n",
      "Loss after mini-batch  1000: 0.844\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.750\n",
      "Loss after mini-batch  1000: 0.687\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.624\n",
      "Loss after mini-batch  1000: 0.580\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.535\n",
      "Loss after mini-batch  1000: 0.497\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.460\n",
      "Loss after mini-batch  1000: 0.431\n"
     ]
    }
   ],
   "source": [
    "network = Net()\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs=5\n",
    "#Run the training loop for defined number of epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        #print(inputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = network(inputs) # Perform forward pass\n",
    "        loss = loss_fn(outputs, targets) # Compute loss\n",
    "        loss.backward() # Backprop\n",
    "        optimizer.step() # Optimization\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: Evaluate the model over the test set (see the data preparation cells), compare the result with the previously seen CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00       137\n",
      "       B-eve       0.00      0.00      0.00       111\n",
      "       B-geo       0.09      0.03      0.04     12357\n",
      "       B-gpe       0.08      0.01      0.02      5226\n",
      "       B-nat       0.00      0.00      0.00        69\n",
      "       B-org       0.16      0.00      0.00      6762\n",
      "       B-per       0.08      0.01      0.02      5649\n",
      "       B-tim       0.10      0.02      0.03      6650\n",
      "       I-art       0.00      0.00      0.00       124\n",
      "       I-eve       0.00      0.00      0.00        89\n",
      "       I-geo       0.09      0.01      0.01      2433\n",
      "       I-gpe       0.00      0.00      0.00        55\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.13      0.01      0.01      5545\n",
      "       I-per       0.08      0.02      0.03      5730\n",
      "       I-tim       0.00      0.00      0.00      2110\n",
      "           O       0.85      0.98      0.91    292571\n",
      "\n",
      "    accuracy                           0.83    345639\n",
      "   macro avg       0.10      0.06      0.06    345639\n",
      "weighted avg       0.73      0.83      0.77    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=True)\n",
    "network.eval()\n",
    "predicted = []\n",
    "true_labels = []\n",
    "for inputs, targets in testloader:\n",
    "    outputs = network(inputs)\n",
    "    predicted_tags = torch.argmax(outputs.view(-1, max_len, n_tags), 2)\n",
    "    true_labels.append([tags[t] for t in targets.view(-1) if t != -1])\n",
    "    predicted.append([tags[t] for t in predicted_tags.view(-1) if t != -1][:len(true_labels[-1])])\n",
    "\n",
    "print(classification_report(np.concatenate(true_labels),np.concatenate(predicted),zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**: Modify the model to use GloVe vectors as initial embeddings (hint: see the documentation of the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\">Embedding</a> layer in PyTorch, in particular the *from_pretrained* method), and compare the results to the base one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [10:57, 1.31MB/s]                               \n",
      "100%|| 35177/35178 [00:00<00:00, 41943.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name=\"6B\", dim = 100, max_vectors=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 1.522\n",
      "Loss after mini-batch  1000: 0.833\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.781\n",
      "Loss after mini-batch  1000: 0.750\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.719\n",
      "Loss after mini-batch  1000: 0.706\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.680\n",
      "Loss after mini-batch  1000: 0.665\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.647\n",
      "Loss after mini-batch  1000: 0.630\n"
     ]
    }
   ],
   "source": [
    "\n",
    "network=Net()\n",
    "network.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "num_epochs=5\n",
    "#Run the training loop for defined number of epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        #print(inputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = network(inputs) # Perform forward pass\n",
    "        loss = loss_fn(outputs, targets) # Compute loss\n",
    "        loss.backward() # Backprop\n",
    "        optimizer.step() # Optimization\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00       137\n",
      "       B-eve       0.00      0.00      0.00       111\n",
      "       B-geo       0.14      0.00      0.01     12357\n",
      "       B-gpe       0.00      0.00      0.00      5226\n",
      "       B-nat       0.00      0.00      0.00        69\n",
      "       B-org       0.00      0.00      0.00      6762\n",
      "       B-per       0.12      0.00      0.01      5649\n",
      "       B-tim       0.00      0.00      0.00      6650\n",
      "       I-art       0.00      0.00      0.00       124\n",
      "       I-eve       0.00      0.00      0.00        89\n",
      "       I-geo       0.00      0.00      0.00      2433\n",
      "       I-gpe       0.00      0.00      0.00        55\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.00      0.00      0.00      5545\n",
      "       I-per       0.00      0.00      0.00      5730\n",
      "       I-tim       0.00      0.00      0.00      2110\n",
      "           O       0.85      1.00      0.92    292571\n",
      "\n",
      "    accuracy                           0.85    345639\n",
      "   macro avg       0.06      0.06      0.05    345639\n",
      "weighted avg       0.72      0.85      0.78    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=True)\n",
    "network.eval()\n",
    "predicted = []\n",
    "true_labels = []\n",
    "for inputs, targets in testloader:\n",
    "    outputs = network(inputs)\n",
    "    predicted_tags = torch.argmax(outputs.view(-1, max_len, n_tags), 2)\n",
    "    true_labels.append([tags[t] for t in targets.view(-1) if t != -1])\n",
    "    predicted.append([tags[t] for t in predicted_tags.view(-1) if t != -1][:len(true_labels[-1])])\n",
    "\n",
    "print(classification_report(np.concatenate(true_labels),np.concatenate(predicted),zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little bit better accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
