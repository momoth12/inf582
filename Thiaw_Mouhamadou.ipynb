{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIyjj8TskT_"
      },
      "source": [
        "<center>\n",
        "<h1><br\\></h1>\n",
        "<h1>INF582: INTRODUCTION TO TEXT MINING AND NLP</h1>\n",
        "</center>\n",
        "<center>\n",
        "<h2>Lab Session 1: TF-IDF</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Dr Guokan Shang and Hadi Abdine <br> contact: hadi.abdine@polytechnique.edu</h4>\n",
        "<h5>Friday, January 12, 2024</h5>\n",
        "<h4><b>Student Name:</b> </h4> THIAW Mouhamadou Lamine Bara\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit to Moodle a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing a your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>January 19, 2024 08:29 AM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "<h3><b>1. Introduction</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Documents are traditionally represented with the vector space model, also known as the bag-of-words representation [<a href='https://nlp.stanford.edu/IR-book/' >Manning et al.</a>]. With this approach, each document di from the collection D = {d1 . . . dm } of size m is associated with an n-dimensional feature vector, where n is the number of unique terms in the preprocessed collection. The set of unique terms T = {t1 . . . tn } is called the vocabulary.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF), is a method used to compute the coordinates of a document in the vector space.\n",
        "</p>\n",
        "\n",
        "<h3><b>2. Learning Objective</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this lab, you will learn how to compute the TF-IDF representation and use it in 3 different tasks:\n",
        "<ul>\n",
        "<li>computing the cosine similarity between documents</li>\n",
        "<li>executing a query against a collection of documents using the inverted index,</li>\n",
        "<li>performing supervised document classification.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<h3><b>3. Computing TF-IDF</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "The assumption is that the importance of a word to a document increases when its frequency increases. However, considering the frequency as the only factor to judge the importance of a word would result in giving greater weight to commonly used terms such as stopwords, which could be misleading in many tasks. TF-IDF mitigates this problem by introducing a factor that diminishes the weight of words that occur frequently in other documents in the same collection. The TF-IDF weight computation is based on the product of two separate factors, namely the Term Frequency (TF) and the Inverse Document Frequency (IDF). More specifically:\n",
        "\n",
        "$$ tf\\text{-}idf(t,d,\\mathcal{D}) = tf(t,d) \\times idf(t, \\mathcal{D}) $$\n",
        "\n",
        "There are many ways to determine tf and idf . In our case, $tf (t, d)$ is the number of times term $t$ appears in document $d$, and $idf (t, D) = ln (\\frac{m}{1+df (t)}) + 1$, with $df (t)$ the number of documents in the collection $D$ that contains $t$. We can notice that the weight of a term in a document increases when its frequency increases in this document (first factor), and decreases when the number of documents in the collection containing this term increases (second factor). M ∈ $R^{m×n}$ is the TF-IDF matrix of $D$, where $M_{ij}$ is the TF-IDF weight of the $jth$ word in the $ith$ document.\n",
        "\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU8DZ-kVPuBX"
      },
      "source": [
        "### Importing libraries and downloading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6fcO2sbrulEJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('webkb-test-stemmed.txt', <http.client.HTTPMessage at 0x19d0c781db0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152573&authkey=AFz5kPjESCHRl3s\", 'webkb-train-stemmed.txt')\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152576&authkey=ACypGA77xWokzQ8\", 'webkb-test-stemmed.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9_vD4yCkGv"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "The next cell implements a <i>tfidf</i> class. When initialized, a <i>tfidf</i> object takes as input a collection of documents and computes the <i>IDF</i> of words in this collection. You can provide a list of stopwords to be ignored. Two methods are defined in this class: <i>tf</i> that returns the frequency of unique words in a document and transform that returns the <i>TF-IDF</i> matrix of the collection. Fill in the gaps in the different functions of <i>tfidf</i> class.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kaN3aNU_svtv"
      },
      "outputs": [],
      "source": [
        "documents = [\"euler is the father of graph theory\",\n",
        "             \"graph theory studies the properties of graphs\",\n",
        "             \"bioinformatics studies the application of efficient algorithms in the biological field\"]\n",
        "\n",
        "class tfidf(object):\n",
        "    def __init__(self, collection, stop_words=[]):\n",
        "        '''collection is a list of strings'''\n",
        "        self.word2ind = {} # map each word in the collection to a unique index\n",
        "        self.ind2word = {}\n",
        "        self.word2idf = {} # map each word to its idf\n",
        "        self.collection = collection\n",
        "        self.documents = [document.split() for document in collection]\n",
        "\n",
        "\n",
        "        self.unique_words = list(set([word for document  in self.documents for word in document])) # fill it, list of unique words in the collection\n",
        "        self.unique_words = [word for word in self.unique_words if word not in stop_words] # remove stopwords\n",
        "        self.count_unique_words = len(self.unique_words)\n",
        "        self.word2ind = dict(zip(self.unique_words,range(self.count_unique_words)))\n",
        "        self.ind2word = {v:k for k,v in self.word2ind.items()}\n",
        "        self.count_documents = len(collection)\n",
        "\n",
        "        # compute the idf of unqiue words in the collection\n",
        "        for word in self.word2ind.keys():\n",
        "            count = sum(1 for document in self.documents if word in document) # fill it, number of documents that contains word\n",
        "            idf = 1+ np.log(   (self.count_documents)  /   (1+count)) # fill it\n",
        "            self.word2idf[word] = idf\n",
        "\n",
        "    def getWordFromInd(self, ind):\n",
        "        return self.ind2word[ind]\n",
        "\n",
        "    def getListWords(self):\n",
        "        return self.unique_words\n",
        "\n",
        "    def tf(self, document):\n",
        "        '''\n",
        "        return the frequency of each unique word in document.\n",
        "        document is a list of strings\n",
        "        '''\n",
        "        word2frequency = {}\n",
        "        for word in document:\n",
        "            word2frequency[word] = word2frequency.get(word, 0) + 1 # increment, creating key if it doesn't already exist\n",
        "        return word2frequency\n",
        "\n",
        "    def transform(self, collection):\n",
        "        documents = [document.split() for document in collection] # tokenize documents in the collection\n",
        "        tfidf_mat = np.zeros(shape=(len(documents), self.count_unique_words)) # fill it, intialize tfidf matrix with zeros\n",
        "        # compute tfidf\n",
        "        for ind, document in enumerate(documents):\n",
        "            word2frequency = self.tf(document)\n",
        "            for word in word2frequency.keys():\n",
        "                if word in self.word2ind:\n",
        "                    tfidf_mat[ind, self.word2ind[word]] = word2frequency[word]*self.word2idf[word] # fill it, tfidf\n",
        "        return tfidf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHf1bwk6GGn0"
      },
      "source": [
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1 (5 points): </b><br>\n",
        "In the above script we provide a small collection documents. In order to validate your functions, compute manually the TF-IDF of the word <b>’the’</b> in the last document and check if you get the same result as with your code.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfnpxmskGtfI"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tfidf of 'the' in last document is:\n",
            "1.4246358550964382\n"
          ]
        }
      ],
      "source": [
        "#tfidf of 'the' in last document\n",
        "t = tfidf(documents)\n",
        "tfidf_mat = t.transform(documents)\n",
        "index = t.word2ind['the']\n",
        "print(\"The tfidf of 'the' in last document is:\")\n",
        "print(tfidf_mat[-1][index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u2-8Oc0HQcj"
      },
      "source": [
        "<h3><b>4. Cosine Similarity</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will compute the cosine similarity of two documents using the TF-IDF representation. The similarity concept is crucial in search engines as well as in text mining applications.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathrm{similarity}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\times \\|v_2\\|}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Normally, the cosine similarity ranges from -1 to +1. However, in our case all the entries of the TF-IDF features are positive, thus it ranges from 0 to 1. The cosine similarity matrix is a square matrix representing the similarity between all pairs of documents in a collection. In other words, if S is the similarity matrix, then $S_{ij} = similarity(v_i , v_j)$ where $v_i$ is the TF-IDF vector of the ith document and $v_j$ is the TF-IDF vector of the jth document.\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDPJQZcTIVbx"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Complete the two functions $\\texttt{cosine_similarity()}$ and $\\texttt{cosine_similarity_matrix()}$ in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OXKsoLZ3Gq3a"
      },
      "outputs": [],
      "source": [
        "documents = [\"i like that music\",\n",
        "\t         \"this song appeals to me\",\n",
        "\t         \"i like graph theory\",\n",
        "\t         \"euler is the father of graph theory\",\n",
        "           \"graph theory studies the properties of graphs\",\n",
        "           \"the quick brown fox jumps over the lazy dog\",\n",
        "           \"the quick fox jumps over the brown lazy dog\"]\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    '''returns the cosine similarity of 2 vectors'''\n",
        "    numerator = np.dot(v1,v2) # fill it\n",
        "    denominator = np.linalg.norm(v1)*np.linalg.norm(v2) # fill it\n",
        "    return numerator / denominator\n",
        "\n",
        "def cosine_similarity_matrix(mat):\n",
        "    '''\n",
        "    returns the cosine similarity matrix\n",
        "    the ith row in mat represents the ith vector\n",
        "    '''\n",
        "    similarity_matrix = np.zeros(shape=(mat.shape[0], mat.shape[0])) # fill it\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[0]):\n",
        "            similarity_matrix[i][j] = cosine_similarity(mat[i],mat[j]) # fill it\n",
        "    return similarity_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBvKViDZJSLN"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2 (2 points): </b><br>\n",
        "Run the code of the next cell and examine its output. What do you observe about the similarity matrix? What can we do to speed-up the computation?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "048fxiP_JSTk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.4845028  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.4845028  0.         1.         0.28294469 0.28294469 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.28294469 1.         0.39794976 0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.28294469 0.39794976 1.         0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "t = tfidf(documents)\n",
        "tfidf_matrix = t.transform(documents)\n",
        "similarity_matrix = cosine_similarity_matrix(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBm-xhaJgoD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "OBSERVATION\n",
        "We can see that:\n",
        "- The similarity matrix is a symetric matrix\n",
        "\n",
        "- The diagonal elements are equal to 1 which is a normal fact because: \n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathrm{similarity}(v_1, v_1) = \\frac{v_1 \\cdot v_1}{\\|v_1\\| \\times \\|v_1\\|} = \\frac{(\\|v_1\\|) ^2}{\\|v_1\\| \\times \\|v_1\\|} = 1\n",
        "\\end{equation}\n",
        "$$\n",
        "- The values range between 0 and 1 which is expected\n",
        "\n",
        "----------------------------------------------------------------------------------------\n",
        "COMPUTATION:\n",
        "To speed up the computation, we can use vectorization and matrix normalization like in the following cell\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.4845028  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.4845028  0.         1.         0.28294469 0.28294469 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.28294469 1.         0.39794976 0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.28294469 0.39794976 1.         0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def cosine_similarity_matrix_vec(mat):\n",
        "    \n",
        "    normalized_matrix = mat/np.linalg.norm(mat, axis=1)[:, np.newaxis]  # Normalisation\n",
        "    similarity_matrix = normalized_matrix @ normalized_matrix.T  \n",
        "    return similarity_matrix\n",
        "\n",
        "t = tfidf(documents)\n",
        "tfidf_matrix = t.transform(documents)\n",
        "similarity_matrix = cosine_similarity_matrix_vec(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRJaBc2M6Rp"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3 (3 points): </b><br>\n",
        "Notice the similarity between first two documents and between last two documents and state two drawbacks of the bag-of-words representation.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1kapOTM_qD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "First drawback:\n",
        "The bag-of-words consider each documents as an unordered set of words and not carrying about the order of words and the global structure.\n",
        "It implies a loss of information because also, 2 sentences which have the same meaning and where we do not use the same words may not have high cosine similarity scores. <br>\n",
        "Second drawback:\n",
        "The bag-of-words consider that all words have the same importance  in an sentence, which might not be true.\n",
        "That is to the origin of techniques like TFIDF to prevent it\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7QybieKBht"
      },
      "source": [
        "<h3><b>5. Inverted Index</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Typically, search engines execute queries against a huge document collection.\n",
        "To execute a query we can simply calculate a similarity measure between the query and all the documents in the collection.<br>\n",
        "However, doing so is very inefficient. Instead, it is possible to eliminate a lot of documents from the candidate set by using the inverted index.\n",
        "The inverted index consists of a mapping from words to documents.\n",
        "Each unique word in the vocabulary is mapped to a list containing the documents in which the word appears and the position of the word in these documents. Stemming, which is reducing tokens to a root form, could be useful in this task. For example a stemming algorithm would reduce <i>computing</i>, <i>computers</i> and <i>computation</i> to <i>comput</i> and thus identifying them as one unique token instead of three different ones. In our code we consider two dictionaries to perform the mapping. In one of the dictionaries the words are stemmed, and in the other they are not.\n",
        "Instead of ranking all the documents in the collection, the inverted index allows us to rank only the relevant documents.\n",
        "\n",
        "In our code we provide several functions:\n",
        "<ul>\n",
        "<li><i>at_least_one_unigram()</i>, returns documents containing at least one query word.\n",
        "<li> <i>all_unigrams()</i>, returns documents containing all query words.\n",
        "<li> <i>ngrams()</i>, returns documents containing all query words in the same order as in the query.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o-fHSDc0JhvQ"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "def clean_tokenize(doc):\n",
        "    words = word_tokenize(doc.lower())\n",
        "    return words\n",
        "\n",
        "def index_one_doc(doc,to_stem):\n",
        "    '''\n",
        "    creates dict (tok,positions) for each tok in the document (as term list)\n",
        "    '''\n",
        "    tokpos = dict()\n",
        "    for t_idx,tok in enumerate(doc):\n",
        "        if to_stem:\n",
        "           tok = stemmer.stem(tok)\n",
        "        if tok in tokpos:\n",
        "            tokpos[tok].append(t_idx)\n",
        "        else:\n",
        "            tokpos[tok] = [t_idx]\n",
        "    return tokpos\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "docs = ['The quick brown fox jumps over the lazy dog',\n",
        "        'The brown quick fox jumps over the lazy dog',\n",
        "        'Luke is the mechanical and electrical engineer of the new group',\n",
        "        'Instead of buying a new engine, buy a new car',\n",
        "        'An engineer may design car engines of all sorts',\n",
        "        'Engineers use logic, but not necessarily imagination',\n",
        "        'Logic will take you from A to Z, imagination will take you everywhere.',\n",
        "        'Continuous effort, not strength or intelligence, is the key to \\\n",
        "        unlocking our potential. And curiosity.',\n",
        "        'It’s OK to have your eggs in one basket as long as you control what \\\n",
        "        happens to that basket.'\n",
        "        ]\n",
        "\n",
        "cleaned_docs = []\n",
        "for doc in docs:\n",
        "    to_app = clean_tokenize(doc)\n",
        "    cleaned_docs.append(to_app)\n",
        "\n",
        "# = = = = = = = = = = = = = = =\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=True)\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=False)\n",
        "\n",
        "'''\n",
        "- queries are not case sensitive\n",
        "- we are indexing punctuation marks\n",
        "- we index stopwords and should keep stopwords in the queries (gives more expressivity)\n",
        "'''\n",
        "\n",
        "inverted_index = dict()\n",
        "inverted_index_stem = dict()\n",
        "\n",
        "for d_idx,doc in enumerate(cleaned_docs):\n",
        "\n",
        "    poslists_s = index_one_doc(doc,to_stem=True) # get positions of each token in the doc\n",
        "    for tok,poslist_s in poslists_s.items():\n",
        "        if tok in inverted_index_stem:\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # update\n",
        "        else:\n",
        "            inverted_index_stem[tok] = dict()\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # initialize\n",
        "\n",
        "    poslists = index_one_doc(doc,to_stem=False)\n",
        "    for tok, poslist in poslists.items():\n",
        "        if tok in inverted_index:\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "        else:\n",
        "            inverted_index[tok] = dict()\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = = = = = = =\n",
        "\n",
        "def at_least_one_unigram(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *at least one* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.extend(list(inverted_index[unigram].keys()))\n",
        "    return list(set(to_return))\n",
        "\n",
        "def all_unigrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *all* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.append(set(list(inverted_index[unigram].keys())))\n",
        "        else:\n",
        "            to_return.append(set())\n",
        "            break\n",
        "    to_return = to_return[0].intersection(*to_return)\n",
        "    return list(to_return)\n",
        "\n",
        "def ngrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing all unigrams in same order as the query\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "    candidate_docs = all_unigrams(query,inverted_index)\n",
        "\n",
        "    to_return = []\n",
        "    for doc in candidate_docs:\n",
        "        poslists = []\n",
        "        for unigram in query:\n",
        "            to_append = inverted_index[unigram][doc]\n",
        "            if isinstance(to_append, int):\n",
        "                poslists.append([to_append])\n",
        "            else:\n",
        "                poslists.append(to_append)\n",
        "        # test whether the query words are consecutive\n",
        "        poslists_sub = [[elt-idx for elt in poslist] for idx,poslist in enumerate(poslists)]\n",
        "        if set(poslists_sub[0]).intersection(*poslists_sub):\n",
        "            to_return.append(doc)\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSfqenBLte7"
      },
      "source": [
        "## Queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5acI7y3yVqnH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: engine car\n",
            "-----------------------------------------------\n",
            "docs containing at least one word in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing all the words in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "\n",
            "docs (stemmed) containing at least one word in the query (stemmed):\n",
            "* Luke is the mechanical and electrical engineer of the new group\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "* Engineers use logic, but not necessarily imagination\n",
            "\n",
            "docs (stemmed) containing all the words in the query (stemmed):\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing engine all the words in the query in the same order:\n",
            "********************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = ['engine','car']\n",
        "print('Query: {}'.format(' '.join(query)))\n",
        "print('-----------------------------------------------')\n",
        "\n",
        "docs_index = at_least_one_unigram(query,inverted_index)\n",
        "print('docs containing at least one word in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query,inverted_index)\n",
        "print('docs containing all the words in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "query_stemmed = [stemmer.stem(elt) for elt in query]\n",
        "docs_index = at_least_one_unigram(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing at least one word in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing all the words in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = ngrams(query,inverted_index)\n",
        "print('docs containing engine all the words in the query in the same order:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print('********************************************************')\n",
        "print()\n",
        "\n",
        "tf_idf = tfidf(docs)\n",
        "query = ['new', 'car']\n",
        "tf_idf_query = tf_idf.transform([(' ').join(query)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvlLzsRtLRla"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Fqx4g0guLvED"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (new car) over all documents\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 1.001119613647461 ms\n",
            "\n",
            "Query (new car) over candidates containing at least one of the words\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.0 ms\n"
          ]
        }
      ],
      "source": [
        "############ query over all documents\n",
        "print('Query ({}) over all documents'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "tf_idf_collection = tf_idf.transform(docs) # fill it\n",
        "scores = [cosine_similarity(tf_idf_query[0], doc) for doc in tf_idf_collection]   # fill it, list containing the cosine similarities of the query against all documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))\n",
        "\n",
        "print('')\n",
        "\n",
        "############ query over candidate documents using inverted index\n",
        "print('Query ({}) over candidates containing at least one of the words'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "candidate_docs_index = at_least_one_unigram(query,inverted_index) # fill it\n",
        "candidate_docs = [docs[el] for el in candidate_docs_index]\n",
        "tf_idf_collection = tf_idf.transform(candidate_docs)\n",
        "scores = [cosine_similarity(tf_idf_query[0], doc) for doc in tf_idf_collection] # fill it, list containing the cosine similarities of the query against candidate documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(candidate_docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z8vwmdeM0jv"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 4 (2.5 points): </b><br>\n",
        "Examine and interpret the output of the previous cell.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYhAq72iNMAc"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "The output suggest that there is a difference in the execution time.\n",
        "In fact, the top similar document is the same for both queries, which is highly expected.\n",
        "But, querying over candidate documents using the inverted index seems to be significantly faster compared to querying over all documents.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYNxB_yNMH9"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 5 (2.5 points): </b><br>\n",
        "If we execute a query the naive way, and then we execute it against the documents containing all the words in the query, is it possible to get different results? Motivate your answer\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huKEjZ6NMXr"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "Yes, it is possible!\n",
        "In fact: <br />\n",
        "-Their is an impact in similarity scores. In fact ,documents containing all the words in the query are likely to have a higher cosine similarity with the query, as they match all the query terms. This can result in different ranking orders for documents and potentially different top similar documents.<br />\n",
        "- The naive query may retrieve documents that are loosely related to any single word in the query, leading to broader results.\n",
        "On the other hand, querying against documents containing all words ensures a more specific relevance to the entire query, resulting in potentially fewer but more relevant documents.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R959CcOlNmfn"
      },
      "source": [
        "<h3><b>6. Supervised Classification</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will use the TF-IDF features to perform a supervised classification task. We will work with the WebKB dataset. It features academic webpages belonging to four different categories: (1) project, (2) course, (3) faculty, and (4) students, and contains 2,803 documents for training and 1,396 for testing. Documents have already been preprocessed with stopword removal and Porter stemming. The code that you will work with implements the following steps:\n",
        "\n",
        "<ul>\n",
        "<li>data loading,\n",
        "<li>computation of TF-IDF features for the training set,\n",
        "<li> computation of features for the test set. Note that the documents in the test set are represented in the space made of the unique terms in the training set only (words in the testing set absent from the training set are disregarded).\n",
        "<li>classifier training/testing. Naive Bayes classifier [<a href='https://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf'>McCallum and Nigam, 1998</a>], Multinomial Logistic Regression [<a href='https://www.learningtheory.org/colt2000/papers/CollinsSchapireSinger.pdf'>Collins et al., 2002</a>], Ran- dom Forest Classifier [1] and linear kernel SVM [<a href='https://link.springer.com/article/10.1007/BF00994018'>Cortes and Vapnik 1995</a>, <a href='https://www.researchgate.net/publication/28351286_Text_Categorization_with_Support_Vector_Machines'>Joachims 1998</a>] are compared,\n",
        "<li>get the most/least important words per class.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjbbZSmPVpu"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QaDtszVDNLg8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logisitc Regression accuracy: 0.8810888252148997\n",
            "Random forest accuracy: 0.8345272206303725\n",
            "Naive Bayes accuracy: 0.8173352435530086\n",
            "Linear SVM accuracy: 0.8474212034383954\n",
            "0.22206303724928367\n",
            "\n",
            "Top 10\n",
            "faculty: ufl cours perman recent cpsc associ teach henri fax professor\n",
            "project: laboratori faculti perform project peopl high lab hybrid request group\n",
            "student: address wisc graduat interest home zhang advisor work construct resum\n",
            "course: hui grade structur materi data assign instructor comp syllabu fall\n",
            "Bottom 10\n",
            "faculty: graduat lab student construct advisor homework syllabu move resum\n",
            "project: interest home address fall email offic fax professor scienc\n",
            "student: professor henri perman faculti group fall hybrid comp fax\n",
            "course: research cours interest vision berkelei cpsc person pictur graphic\n"
          ]
        }
      ],
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    # coef stores the weights of each feature (in unique term), for each class\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in top10)))\n",
        "\n",
        "def print_bot10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the lowest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        bot10 = np.argsort(clf.coef_[i])[0:9]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in bot10)))\n",
        "\n",
        "def prepare_data(path):\n",
        "    with open(path, 'r') as f:\n",
        "        s = f.read()\n",
        "    examples = s.split('\\n') # split to samples\n",
        "    examples = examples[:-1] # last element is empty\n",
        "    documents = []\n",
        "    labels = []\n",
        "    for el in examples:\n",
        "        example = el.split('\\t') # separate document from label\n",
        "        documents.append(example[1])\n",
        "        labels.append(example[0])\n",
        "    return documents, labels\n",
        "\n",
        "path_train = './webkb-train-stemmed.txt' # path to train data\n",
        "path_test = './webkb-test-stemmed.txt' # path to test data\n",
        "train_documents, train_labels = prepare_data(path_train)\n",
        "test_documents, test_labels = prepare_data(path_test)\n",
        "\n",
        "categories = set(train_labels) # get unique categoris\n",
        "category2ind = dict(zip(categories,range(len(categories)))) # map each category to index\n",
        "ind2category = {v:k for k,v in category2ind.items()} # map index to category\n",
        "\n",
        "train_labels_index = [category2ind[cat] for cat in train_labels] # replace labels by their indexes\n",
        "test_labels_index = [category2ind[cat] for cat in test_labels] # replace labels by their indexes\n",
        "\n",
        "t = tfidf(train_documents) # fill it, tfidf object\n",
        "tfidf_train = t.transform(train_documents) # fill it, get the tfidf training matrix\n",
        "tfidf_test = t.transform(test_documents) # fill it, get the tfidf text matrix\n",
        "\n",
        "lr = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=200)\n",
        "rf = RandomForestClassifier(n_estimators=20)\n",
        "nb = MultinomialNB()\n",
        "svm = LinearSVC(max_iter=2000)\n",
        "\n",
        "Classifiers = {'Logisitc Regression': lr, 'Random forest': rf, 'Naive Bayes': nb, 'Linear SVM': svm}\n",
        "from sklearn.metrics import accuracy_score\n",
        "for classifier in Classifiers.keys():\n",
        "    Classifiers[classifier].fit(tfidf_train, train_labels_index) # train each classifier\n",
        "    predicted = Classifiers[classifier].predict(tfidf_test) # perform prediction\n",
        "    accuracy = accuracy_score(test_labels_index,predicted) # fill it, compute accuracy\n",
        "    print('{} accuracy: {}'.format(classifier, accuracy))\n",
        "\n",
        "predicted = np.zeros((len(test_labels_index))) + 3\n",
        "print(np.mean(predicted == np.array(test_labels_index)))\n",
        "print('')\n",
        "\n",
        "# choose one classfier and print top 10 and bottom 10 words per class\n",
        "feature_names = t.getListWords() # fill it\n",
        "classifier = lr # fill it\n",
        "print('Top 10')\n",
        "print_top10(feature_names, classifier, categories)\n",
        "print('Bottom 10')\n",
        "print_bot10(feature_names, classifier, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjXDedgQFxn"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 6 (5 points): </b><br>\n",
        "Examine the accuracy of different classifiers, examine the most/least important words and comment the results.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKQ-IEKTQUzD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "While examinating the accuracy , we can see that the logistic regression has the best accuracy score comparing to others classifiers but ther other perform to this task very well. This show his performnce in dealing with classification tasks.\n",
        "some labels are on their own top10 which is normal.\n",
        "And generally, the classifiers demonstrate good accuracy,  the importance of specific words provides good insights into the features contributing to accurate predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
